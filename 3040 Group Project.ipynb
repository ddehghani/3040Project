{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b5a221",
   "metadata": {},
   "source": [
    "<b>Introduction and Motivation</b>\n",
    "\n",
    "Blood glucose level is a decisive factor in determining whether a patient has diabetes, however, other than blood glucose level, the patient’s age, physical conditions, and certain symptoms can potentially help with the diagnosis of diabetes. Early diagnosis of diabetes can increase the likelihood of treatment and can greatly reduce the health complications of the patients. \n",
    "Furthermore, identifying key predictors of diabetes can help doctors prioritize the examinations and diagnose diabetes faster and easier. \n",
    "Our main goal is to make a classifier that can predict diabetes solely based on known symptoms and physical condition of the patients with at least 80% accuracy. Finally, we would also like to produce a model that identifies the key predictors of diabetes. \n",
    "\n",
    "<b>About our Dataset</b>\n",
    "\n",
    "Our chosen data set is ‘The early-stage diabetes risk prediction’. It contains 17 attributes all of which except age are binary. This includes one class attribute which accepts two values: Negative or Positive. The class attribute represents whether or not the patient has diabetes. Our data has 520 instances.\n",
    "\n",
    "<b>Previous Work</b>\n",
    "\n",
    "In the last stage, we focused on data pre-processing, EDA, and reporting preliminary results. \n",
    "\n",
    "<b>Current Problem</b>\n",
    "\n",
    "Our problem is best framed as a binary classification that predicts whether a patient is or soon will be diabetic, based on their symptoms as well as other information like their age and gender.\n",
    "\n",
    "In this stage of the project, we mainly focus on implementing three different algorithms and techniques to classify our diabetes data. Since python has useful libraries for data analysis, we chose it for our project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdcdbb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # mathematical operations and algebra\n",
    "import pandas as pd # data processing, CSV file I/O\n",
    "#import matplotlib.pyplot as plt # visualization library\n",
    "from tree import *\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3199e797",
   "metadata": {},
   "source": [
    "After importing the required libraries, we import the dataset from the CSV file into a padas data frame for further processing.\n",
    "\n",
    "<b>IMPORTING DATA</b>\n",
    "\n",
    "Objective:\n",
    "<ul>\n",
    "<li> Import data from CSV file into a padas DataFrame.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "835a47d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv (r'diabetes_data_upload.csv')\n",
    "labels = ['Age','Gender','Polyuria','Polydipsia','sudden weight loss','weakness','Polyphagia',\n",
    "'Genital thrush','visual blurring','Itching','Irritability','delayed healing','partial paresis',\n",
    "'muscle stiffness','Alopecia','Obesity','class']\n",
    "df = pd.DataFrame(data, columns= labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d244a80f",
   "metadata": {},
   "source": [
    "After all the data is imported, we proceed to data cleaning. In this process, we will make sure our dataset is free of all missing/ incomplete values, duplicate data, noise, outliers and wrong data as much as possible.\n",
    "\n",
    "<b>DATA CLEANING</b>\n",
    "\n",
    "Objective:\n",
    "<ul>\n",
    "<li> Check if the data contains any null, missing, duplicate.</li>\n",
    "<li> If yes take appopriate action.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fc026a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values in data set: True\n"
     ]
    }
   ],
   "source": [
    "print(f'No missing values in data set: {not df.isnull().values.any()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f11e936",
   "metadata": {},
   "source": [
    "Since we have no missing values, no action is required. Our data is clean and we can proceed with the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf82f488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of dumplicated instances (counting the original) is 376\n",
      "The total number of dumplicated instances  (not counting the original) is: 269\n"
     ]
    }
   ],
   "source": [
    "# duplicate data\n",
    "\n",
    "print(f'The total number of dumplicated instances (counting the original) is {sum(df.duplicated(keep=False))}')\n",
    "print(f'The total number of dumplicated instances  (not counting the original) is: {sum(df.duplicated())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845143f",
   "metadata": {},
   "source": [
    "At first it seems like 269 out of 520 is a lot of duplicated values.\n",
    "Lets drop age as it is an obvious tie breaker between two rows. Then recalculate the duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0574eb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate instances counting the original is 407\n",
      "Number of duplicate instances NOT counting the original is 305\n"
     ]
    }
   ],
   "source": [
    "df_without_age = df.drop(['Age'], axis=1)\n",
    "print(f'Number of duplicate instances counting the original is {sum(df_without_age.duplicated(keep=False))}')\n",
    "print(f'Number of duplicate instances NOT counting the original is {sum(df_without_age.duplicated())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e336eb0d",
   "metadata": {},
   "source": [
    "It's a lot more now (as expected) so let's take a look at some of these duplicate values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b28e5fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age Gender Polyuria Polydipsia sudden weight loss weakness Polyphagia  \\\n",
      "374   27   Male       No         No                 No       No         No   \n",
      "286   27   Male       No         No                 No       No         No   \n",
      "465   27   Male       No         No                 No       No         No   \n",
      "474   27   Male       No         No                 No       No         No   \n",
      "\n",
      "    Genital thrush visual blurring Itching Irritability delayed healing  \\\n",
      "374             No              No      No           No              No   \n",
      "286             No              No      No           No              No   \n",
      "465             No              No      No           No              No   \n",
      "474             No              No      No           No              No   \n",
      "\n",
      "    partial paresis muscle stiffness Alopecia Obesity     class  \n",
      "374              No               No       No      No  Negative  \n",
      "286              No               No       No      No  Negative  \n",
      "465              No               No       No      No  Negative  \n",
      "474              No               No       No      No  Negative  \n"
     ]
    }
   ],
   "source": [
    "duplicated_data = df[df.duplicated()].sort_values(by='Age')\n",
    "print(duplicated_data[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36fc7b8",
   "metadata": {},
   "source": [
    "So most of these instances are simply copies of a few 'common' cases. This can be illustrated if we take a look at some of these instances.\n",
    "\n",
    "We concluded that our data is clean and free of outliers, and those so-called duplicates are not actual duplicates but just a naturally higher rate of occurrence of likely cases. For example, people age 27 with no health conditions or symptoms are common, and this is to be expected.\n",
    "\n",
    "After all the preprocessing work is done, we can implement 3 classification algorithms one by one to our dataset. \n",
    "\n",
    "<br>\n",
    "<b>DECISION TREE METHOD</b>\n",
    "\n",
    "Objectives:\n",
    "<ul>\n",
    "<li> Create a decision tree and display it as a graph.</li> \n",
    "<li>  Evaluate its accuracy of this tree by testing it with a random subset of the data that wasn't a part of training data.</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7924a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretize the age into a few categories\n",
    "df_discretize = df.copy(deep=True)\n",
    "minAge = df_discretize.Age.min()\n",
    "maxAge = df_discretize.Age.max()\n",
    "range = maxAge - minAge\n",
    "df_discretize.Age = pd.cut(df[\"Age\"],\n",
    "       bins=[minAge, minAge + range/3, minAge + 2*range/3, maxAge], \n",
    "       labels=[\"Young\", \"Adult\", \"Old\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c64c78",
   "metadata": {},
   "source": [
    "We discretize age since it was a continuous attribute and a decision tree can be built using discrete attributes only.\n",
    "\n",
    "We built our model based on many different approaches to discretization. Practically, we found the equal-width binning approach to yield the most accurate result.\n",
    "\n",
    "We tested different bin sizes but 3 seemed to work best with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f601091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the discretized data into training and test \n",
    "df_discretize_test = df_discretize.sample(n = 50, replace = False) # change test number from here\n",
    "df_discretize_training = df_discretize.copy(deep=True)\n",
    "df_discretize_training = df_discretize_training.drop(df_discretize_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35a06db",
   "metadata": {},
   "source": [
    "We separated the instances into training and test. Out of 520 instances, 50 are randomly selected for test and the rest are our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dacd7f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.98\n"
     ]
    }
   ],
   "source": [
    "# decision Tree\n",
    "root = buildDecisionTree(data=df_discretize_training, classAttribute='class')\n",
    "# draw it as a graph\n",
    "buildGraph(root).view()\n",
    "# calculate its accuracy (evaluation using test data)\n",
    "print(\"Accuracy: \",evaluateTree(root,df_discretize_test,'class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c035b6f",
   "metadata": {},
   "source": [
    "<b>KNN METHOD</b>\n",
    "\n",
    "Objectives:\n",
    "<ul>\n",
    "<li> Classify test data using KNN method</li> \n",
    "<li>  Evaluate the accuracy of this method by testing it with a random subset of the data that wasn't a part of training data.</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd90ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize age (max min)\n",
    "normalized_age = (df.Age - df.Age.min()) / (df.Age.max() - df.Age.min())\n",
    "df_normalized = df.copy(deep=True)\n",
    "df_normalized.Age = normalized_age "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3895d572",
   "metadata": {},
   "source": [
    "We normalized the age since we don't want it to dominate the distance. Other attributes are binary and no normalization is needed.\n",
    "\n",
    "Then just like before, we separated the instances into training and test. Out of 520 instances, 50 are randomly selected for test and the rest are our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7987fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the normalized data into training and test \n",
    "df_normalized_test = df_normalized.sample(n = 50, replace = False) # change test number from here\n",
    "df_normalized_training = df_normalized # no copy is needed because df_normalized is never used again\n",
    "df_normalized_training = df_normalized_training.drop(df_normalized_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ba2fb",
   "metadata": {},
   "source": [
    "To compute the distance between two data points, the euclidean distance of their Age is calculated. For other attributes, the binary distance is calculated. We then sum the two distances and divide them by two to get the total distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32f72886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.92\n"
     ]
    }
   ],
   "source": [
    "attr_labels = ['Gender', 'Polyuria','Polydipsia','sudden weight loss','weakness','Polyphagia',\n",
    "'Genital thrush','visual blurring','Itching','Irritability','delayed healing','partial paresis',\n",
    "'muscle stiffness','Alopecia','Obesity']\n",
    "success = 0\n",
    "for index, data in df_normalized_test.iterrows():\n",
    "    distance = 0\n",
    "    for label in attr_labels:\n",
    "        distance += df_normalized_training[label] != data[label]\n",
    "    distance = distance / len(attr_labels) \n",
    "    distance += ((df_normalized_training.Age - data.Age) ** 2)**(1/2)\n",
    "    distance = distance / 2\n",
    "    df_normalized_training['distance'] = distance\n",
    "    knn = df_normalized_training.sort_values(by=['distance']).head(10) # change k value here\n",
    "    # print(knn) #uncomment to see the k nearest neighbours\n",
    "    if knn['class'].value_counts().idxmax() == data['class']:\n",
    "        success += 1\n",
    "\n",
    "print(\"Accuracy: \", success / df_normalized_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a035622",
   "metadata": {},
   "source": [
    "<b>BAYES METHOD</b>\n",
    "\n",
    "Objectives:\n",
    "<ul>\n",
    "<li> Classify test data using BAYES method</li> \n",
    "<li>  Evaluate the accuracy of this method by testing it with a random subset of the data that wasn't a part of training data.</li> \n",
    "</ul>\n",
    "<br>\n",
    "We can use the same training and test data that we used for decision tree method here because we need discrete attributes for this method as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36a05e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.72\n"
     ]
    }
   ],
   "source": [
    "def prob(data, value, attr, givenValue = None, givenAttr = None): # fuction for probability calculation \n",
    "    if (givenValue is not None and givenAttr is not None):\n",
    "        data = data[data[givenAttr] == givenValue]\n",
    "    value_count = data[attr].value_counts()\n",
    "    return value_count[value]/sum(value_count)\n",
    "\n",
    "labels = ['Age', 'Gender', 'Polyuria','Polydipsia','sudden weight loss','weakness','Polyphagia',\n",
    "'Genital thrush','visual blurring','Itching','Irritability','delayed healing','partial paresis',\n",
    "'muscle stiffness','Alopecia','Obesity']\n",
    "success = 0\n",
    "p_positive = prob(df_discretize_training, 'Positive', 'class') \n",
    "p_negative = 1 - p_positive\n",
    "for index, data in df_discretize_test.iterrows():\n",
    "    for label in labels:\n",
    "        value = data[label]\n",
    "        p_positive *= prob(df_discretize_training, value, label, 'Positive', 'class')\n",
    "        p_negative *= prob(df_discretize_training, value, label, 'Negative', 'class')\n",
    "    classification = \"Positive\" if p_positive > p_negative else \"Negative\"\n",
    "    if data['class'] == classification:\n",
    "        success += 1\n",
    "\n",
    "print(\"Accuracy: \", success / df_discretize_test.shape[0])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba28b4",
   "metadata": {},
   "source": [
    "The accuracy of our algorithm is generated dynamically, it depends on which samples are fetched in this run time. We are showing the best result returned by python and there is a possibility that the accuracy can be higher.\n",
    "Until now, we have compared 3 different algorithms in terms of accuracy to classify our data set. Out of curiosity, we also use Weka as a reference to compare with our algorithms. Based on the property of Weka, the accuracy output can be considered a standard criterion. Weka is setting default with 10 folds cross-validation.\n",
    "\n",
    "<br><center><b>Decision tree – J48 algorithm (95% accuracy)</b></center><br>\n",
    "<center><img src=\"images/dteval.jpg\" style=\"height: 700px; width:900px;\"></center>\n",
    "\n",
    "<center><img src=\"images/tree.jpg\" style=\"height: 700px; width:900px;\"></center>\n",
    "\n",
    "<br><center><b>KNN algorithm – Lazy IBK (90% accuracy)</b></center><br>\n",
    "<center><img src=\"images/knn.jpg\" style=\"height: 700px; width:900px;\"></center>\n",
    "\n",
    "<br><center><b>Bayes – NaiveBayes (87% accuracy)</b></center><br>\n",
    "<center><img src=\"images/bayes.jpg\" style=\"height: 700px; width:900px;\"></center>\n",
    "\n",
    "By comparing our algorithms with Weka, we can tell that our performance is almost the same in terms of decision tree and KNN algorithms. In Naïve Bayes, the Weka has slightly higher accuracy due to the more advanced algorithms, we can probably do better with that the next time. \n",
    "In terms of accuracy evaluation, we also compare the time complexity of our different algorithms. Bayes runs fastest because it only involves simple calculation, while the decision tree runs slower due to it has to build models for each attribute. KNN is the lowest one because it takes time to let the code decide how to group instances and decide which area it wants to choose. For boosting, since we’ve already done data cleaning and preprocessing, no missing values lead to the same process time in terms of boosting algorithm. \n",
    "\n",
    "In conclusion, we cleaned data and import it to python to process, and implement decision tree, KNN, and Bayes algorithms to classify the diabetes dataset with an accuracy rate of 96%, 90%, and 72% respectively.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
